{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "import torchvision as tv\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.offsetbox import OffsetImage, AnnotationBbox\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.manifold import TSNE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Configuration "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ROOT_DIR = \"\"\n",
    "IMG_SIZE = 64\n",
    "BATCH_SIZE = 128\n",
    "LATENT_DIMS = 16\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_csv = pd.read_csv(ROOT_DIR + \"Train.csv\")\n",
    "test_csv = pd.read_csv(ROOT_DIR + \"Test.csv\")\n",
    "\n",
    "train_files = train_csv[[\"Path\", \"ClassId\"]]\n",
    "test_files = test_csv[[\"Path\", \"ClassId\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfms = tv.transforms.Compose([tv.transforms.Resize((IMG_SIZE, IMG_SIZE)), tv.transforms.ToTensor()])\n",
    "filenames = [os.path.join(dirpath,filename) for dirpath, _, filenames in os.walk(ROOT_DIR + \"Train/\") for filename in filenames if filename.endswith('.png')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data into memory\n",
    "\n",
    "file_arr = []\n",
    "for i in tqdm(range(len(filenames))):\n",
    "    image = Image.open(filenames[i])\n",
    "    tens = tfms(image)\n",
    "    conv_filename = filenames[i].split(\"gtsrb/\")[-1]\n",
    "    class_id = int(train_files[train_files[\"Path\"] == conv_filename][\"ClassId\"].astype(int))\n",
    "    tens_id_arr = [tens, class_id]\n",
    "    file_arr.append(tens_id_arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make sure that classes are mixed before splitting array into train and validation set\n",
    "\n",
    "random.shuffle(file_arr)\n",
    "\n",
    "train_files = file_arr[:-1000]\n",
    "valid_files = file_arr[-1000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TSDataset(Dataset):\n",
    "    def __init__(self, files, transform=None):\n",
    "        self.files = files\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "            \n",
    "        x = self.files[idx][0]\n",
    "        label = self.files[idx][1]\n",
    "            \n",
    "        return x, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = TSDataset(train_files, tfms)\n",
    "valid_data = TSDataset(valid_files, tfms) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(training_data, batch_size=BATCH_SIZE, shuffle=True)\n",
    "# shuffle = false to be able to compare output(-improvements) during training\n",
    "valid_dataloader = DataLoader(valid_data, batch_size=BATCH_SIZE, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classifier architecture\n",
    "\n",
    "class Classifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Classifier, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 10, kernel_size=5)\n",
    "        self.conv2 = nn.Conv2d(10, 20, kernel_size=5)\n",
    "        self.conv3 = nn.Conv2d(20, 20, kernel_size=5)\n",
    "        self.conv2_drop = nn.Dropout2d()\n",
    "        self.fc1 = nn.Linear(320, 50)\n",
    "        self.fc2 = nn.Linear(50, 43)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(F.max_pool2d(self.conv1(x), 2))\n",
    "        x = F.relu(F.max_pool2d(self.conv2_drop(self.conv2(x)), 2))\n",
    "        x = F.relu(F.max_pool2d(self.conv2_drop(self.conv3(x)), 2))\n",
    "        x = x.view(-1, 320)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.dropout(x, training=self.training)\n",
    "        x = self.fc2(x)\n",
    "        return F.log_softmax(x, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Flatten(nn.Module):\n",
    "    def forward(self, input):\n",
    "        return input.view(input.size(0), -1)\n",
    "\n",
    "\n",
    "class UnFlatten(nn.Module):\n",
    "    def forward(self, input, size=1024):\n",
    "        return input.view(input.size(0), size, 1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.kaggle.com/code/muhammad4hmed/anime-vae/notebook\n",
    "\n",
    "class CVAE(nn.Module):\n",
    "    def __init__(self, image_channels=3, h_dim=1024, z_dim=16):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv2d(image_channels, 32, kernel_size=4, stride=2),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 64, kernel_size=4, stride=2),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 128, kernel_size=4, stride=2),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(128, 256, kernel_size=4, stride=2),\n",
    "            nn.ReLU(),\n",
    "            Flatten()\n",
    "        )\n",
    "        \n",
    "        self.signclass_embedding = nn.Embedding(43, 10)\n",
    "        \n",
    "        self.h2mu = nn.Linear(h_dim, z_dim)\n",
    "        self.h2sigma = nn.Linear(h_dim, z_dim)\n",
    "        self.z2h = nn.Linear(z_dim + 10, h_dim)\n",
    "        \n",
    "        self.decoder = nn.Sequential(\n",
    "            UnFlatten(),\n",
    "            nn.ConvTranspose2d(h_dim, 128, kernel_size=5, stride=2),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(128, 64, kernel_size=5, stride=2),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(64, 32, kernel_size=6, stride=2),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(32, image_channels, kernel_size=6, stride=2),\n",
    "            nn.Sigmoid(),\n",
    "        )\n",
    "        \n",
    "    # Enforce latent space well-formedness by jinecting random gaussian noise    \n",
    "    def reparameterize(self, mu, logvar):          \n",
    "        std = logvar.mul(0.5).exp_()\n",
    "        eps = torch.randn(*mu.size()).to(DEVICE)\n",
    "        z = mu + std * eps\n",
    "        return z\n",
    "    \n",
    "    def bottleneck(self, h, label):\n",
    "        mu = self.h2mu(h)\n",
    "        logvar = self.h2sigma(h)\n",
    "        z = self.reparameterize(mu, logvar)\n",
    "        return z, mu, logvar\n",
    "        \n",
    "    def encode(self, x, label):\n",
    "        return self.bottleneck(self.encoder(x), label)[0]\n",
    "\n",
    "    def decode(self, z):\n",
    "        return self.decoder(self.z2h(z))\n",
    "    \n",
    "    def forward(self, x, label):\n",
    "        h = self.encoder(x)\n",
    "        z_small, mu, logvar = self.bottleneck(h, label)     \n",
    "        signclass = self.signclass_embedding(label.long())\n",
    "        signclass = signclass.squeeze(dim=1)\n",
    "        z_small_cat = torch.cat([z_small, signclass], dim=1)\n",
    "        z = self.z2h(z_small_cat)\n",
    "        return self.decoder(z), mu, logvar, z_small, z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensemble architecture (combining cvae and classifier)\n",
    "\n",
    "class Ensemble(nn.Module):\n",
    "    def __init__(self, embeddings, upscaler, decoder, classifier):\n",
    "        super(Ensemble, self).__init__()\n",
    "        self.embeddings = embeddings\n",
    "        self.upscaler = upscaler\n",
    "        self.decoder = decoder\n",
    "        self.classifier = classifier\n",
    "        \n",
    "    def forward(self, z, label):\n",
    "        enc_label = self.embeddings(label.long())\n",
    "        enc_label = enc_label.squeeze(dim=1)\n",
    "        x = torch.cat((z, enc_label), dim=1)\n",
    "        x = self.upscaler(x)\n",
    "        x = self.decoder(x)\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "    \n",
    "    def get_img(self, z, label):\n",
    "        enc_label = self.embeddings(label.long())\n",
    "        x = torch.cat((z, enc_label), dim=1)\n",
    "        x = self.upscaler(x)\n",
    "        x = self.decoder(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pre-trained models for classifier and cvae\n",
    "\n",
    "classifier = Classifier()\n",
    "cvae = CVAE()\n",
    "\n",
    "classifier.eval()\n",
    "cvae.eval()\n",
    "\n",
    "classifier.load_state_dict(torch.load(\"...\"))\n",
    "cvae.load_state_dict(torch.load('...'))\n",
    "\n",
    "classifier.to(DEVICE)\n",
    "cvae.to(DEVICE);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load cvae and classifier into ensemble\n",
    "\n",
    "embeddings, upscaler, decoder = cvae.extract_model()\n",
    "ensemble = Ensemble(embeddings, upscaler, decoder, classifier)\n",
    "ensemble.to(DEVICE);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate adversarial example\n",
    "# https://adversarial-ml-tutorial.org/adversarial_training/\n",
    "\n",
    "def pgd_linf(model, X, y, epsilon, alpha, num_iter):\n",
    "    delta = torch.zeros_like(X, requires_grad=True)\n",
    "    for t in range(num_iter):\n",
    "        pred = model(X + delta, y)\n",
    "        loss = nn.CrossEntropyLoss()(pred, y.squeeze(dim=1))\n",
    "        loss.backward(retain_graph=True)\n",
    "        delta.data = (delta + alpha*delta.grad.detach().sign()).clamp(-epsilon,epsilon)\n",
    "        delta.grad.zero_()\n",
    "    return delta.detach()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cvae_data, cvae_labels = next(iter(train_dataloader))\n",
    "cvae_data, cvae_labels = cvae_data.to(DEVICE), cvae_labels.to(DEVICE)\n",
    "cvae_labels = cvae_labels.unsqueeze(dim=1)\n",
    "cvae_data.shape, cvae_labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recon_batch, mu, logvar, data, z = cvae(cvae_data, cvae_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualize the number of adversaries per class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample 1024 uniformly distributed scene vectors\n",
    "\n",
    "uniform_dist = (-2-2) * torch.rand_like(mu) + 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For all sign classes, calculate 1024 adversarial perturbations from random starting points in [-2,2]\n",
    "\n",
    "all_danger = torch.zeros((43, 1024))\n",
    "\n",
    "for i in tqdm(range(43)):\n",
    "    cvae_labels = [i]\n",
    "    cvae_labels = torch.Tensor(cvae_labels).unsqueeze(dim=1)\n",
    "    cvae_labels = cvae_labels.expand(BATCH_SIZE, -1)\n",
    "    cvae_labels = cvae_labels.type(torch.LongTensor)\n",
    "    cvae_labels = cvae_labels.to(DEVICE)\n",
    "\n",
    "    mu2 = uniform_dist\n",
    "    recon_batch, mu, logvar, data, z = cvae(cvae_data, cvae_labels)\n",
    "    delta = pgd_linf(ensemble, mu2, cvae_labels, epsilon=0.2, alpha=2e-2, num_iter=50)\n",
    "    yp = ensemble(mu2 + delta, cvae_labels)\n",
    "    prob_orig = F.softmax(yp)\n",
    "\n",
    "    danger_array = prob_orig.gather(1, cvae_labels).squeeze(dim=1)\n",
    "    danger_array = danger_array.to(\"cpu\").detach()\n",
    "    danger_array = danger_array.unsqueeze(dim=0)\n",
    "    \n",
    "    all_danger[i] = danger_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Order the previsouly calculated p(correct_class) \n",
    "\n",
    "ordered = torch.zeros((43, 1024))\n",
    "danger_order = torch.argsort(all_danger, dim=1)\n",
    "top = danger_order[:,:1024]\n",
    "\n",
    "for i in range(43):\n",
    "    top_danger = all_danger[i, top[i]]\n",
    "    ordered[i] = top_danger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the effect that adversarial perturbations have on the classifier's performance\n",
    "\n",
    "selected_class = 10\n",
    "plt.plot(ordered[selected_class], label=\"\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Failure Mode Computing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Option-1: Create label-vector (LOCAL)\n",
    "\n",
    "selected_class = 10\n",
    "\n",
    "cvae_labels = [selected_class]\n",
    "cvae_labels = torch.Tensor(cvae_labels).unsqueeze(dim=1)\n",
    "cvae_labels = cvae_labels.expand(BATCH_SIZE, -1)\n",
    "cvae_labels = cvae_labels.type(torch.LongTensor)\n",
    "cvae_labels = cvae_labels.to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Option-2: Create label-vector (GLOBAL)\n",
    "\n",
    "cvae_labels = []\n",
    "cvae_labels.extend(range(0, 43))\n",
    "cvae_labels = torch.Tensor(cvae_labels).unsqueeze(dim=1)\n",
    "cvae_labels = cvae_labels.type(torch.LongTensor)\n",
    "cvae_labels = cvae_labels.to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Option-1: Calculate p(correct_class) for BATCH_SIZE adversaries (LOCAL)\n",
    "\n",
    "mu2 = uniform_dist\n",
    "delta = pgd_linf(ensemble, mu2, cvae_labels, epsilon=0.2, alpha=2e-2, num_iter=50)\n",
    "yp = ensemble(mu2 + delta, cvae_labels)\n",
    "prob_orig = F.softmax(yp)\n",
    "\n",
    "mu_adv = mu2 + delta\n",
    "\n",
    "danger_array = prob_orig.gather(1, cvae_labels).squeeze(dim=1)\n",
    "l2_array = torch.linalg.norm(mu2, dim=1, ord=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Option-2: Calculate p(correct_class) for BATCH_SIZE adversaries (GLOBAL)\n",
    "\n",
    "danger_array = []\n",
    "l2_array = []\n",
    "delta_array = []\n",
    "\n",
    "for i in tqdm(range(BATCH_SIZE))\n",
    "    mu2 = uniform_dist[i].expand(43, -1)\n",
    "    delta = pgd_linf(ensemble, mu2, cvae_labels, epsilon=0.2, alpha=2e-2, num_iter=50)\n",
    "    yp = ensemble(mu2 + delta, cvae_labels)\n",
    "    prob_orig = F.softmax(yp)\n",
    "    \n",
    "    danger = sum(prob_orig.gather(1, cvae_labels))[0].item()/43\n",
    "    l2_norm = torch.linalg.norm(mu[i], dim=0, ord=2)\n",
    "    \n",
    "    l2_array.append(l2_norm.item())\n",
    "    danger_array.append(danger)\n",
    "    delta_array.append(delta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select most dangerous 200 styles (LOCAL & GLOBAL) \n",
    "\n",
    "d_tensor = torch.Tensor(danger_array)\n",
    "l2_tensor = torch.Tensor(l2_array)\n",
    "normed_tensor = d_tensor*l2_tensor\n",
    "\n",
    "danger_order = torch.argsort(d_tensor, dim=0)\n",
    "top = danger_order[:200]\n",
    "\n",
    "mu_sel = mu[top]\n",
    "mu_sel = mu_sel.to(\"cpu\").detach().numpy() \n",
    "d_sel = d_tensor[top]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply k-Means to the 200 most dangerous perturbed scenes\n",
    "\n",
    "k = 4\n",
    "kmeans = KMeans(n_clusters=k, random_state=42).fit(mu_sel)\n",
    "labels = kmeans.labels_\n",
    "clusts = torch.Tensor(kmeans.cluster_centers_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_images(X, y, yp, N):\n",
    "    fig = plt.figure()\n",
    "    for j in range(N):\n",
    "        a = fig.add_subplot(1,4,j+1)\n",
    "        a.imshow(X[j])\n",
    "        a.set_axis_off()\n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the k failure modes\n",
    "\n",
    "yp = ensemble(clusts, cvae_labels[:k])\n",
    "\n",
    "imgs = ensemble.get_img(clusts, cvae_labels[:k].squeeze(dim=1))\n",
    "imgs = imgs.detach().cpu().numpy()\n",
    "imgs = imgs.transpose(0, 2, 3, 1)\n",
    "plot_images(imgs, cvae_labels[:k], yp, 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# t-SNE Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tsne_results = TSNE(n_components=2, verbose=1, metric='euclidean').fit_transform(mu_sel);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assign color to p(correct_label); 0.00 = red, 1.00 = green, ...\n",
    "\n",
    "colorscale = [\"#F50E00\", \"#E62D00\", \"#D94800\", \"#C66D00\", \"#B98700\", \"#B98700\", \"#A6AE00\", \"#97CD00\", \"#88EB00\", \"#80FA00\"]\n",
    "def assign_color(prob):\n",
    "    pct_val = int(((prob*100)/10)-1)\n",
    "    pct_val = max(pct_val, 0)\n",
    "    return colorscale[pct_val]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot images in t-SNE grid\n",
    "\n",
    "def plot_images_in_2d(x, y, image_idxs, axis=None, zoom=1):\n",
    "    if axis is None:\n",
    "        axis = plt.gca()\n",
    "    x, y = np.atleast_1d(x, y)\n",
    "    for x0, y0, idx in zip(x, y, image_idxs):\n",
    "        style = uniform_dist[idx]\n",
    "        style = style.unsqueeze(dim=0)\n",
    "        # Select a class\n",
    "        label = torch.Tensor([10])\n",
    "        \n",
    "        img = ensemble.get_img(style, label.squeeze(dim=1))\n",
    "        imgs = img.detach().cpu().numpy()\n",
    "        imgs = imgs.transpose(0, 2, 3, 1)\n",
    "        imgs = imgs[0]\n",
    "        imgs = OffsetImage(imgs, zoom=zoom)\n",
    "        anno_box = AnnotationBbox(imgs, (x0, y0),\n",
    "                                  xycoords='data',\n",
    "                                  frameon=False)\n",
    "        axis.annotate(\"{:.2f}\".format(d_tensor[idx]), color=assign_color(d_tensor[idx]), xy=(x0-17,(y0+40)))\n",
    "        axis.add_artist(anno_box)\n",
    "    axis.update_datalim(np.column_stack([x, y]))\n",
    "    axis.autoscale()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create t-SNE-grid\n",
    "# https://github.com/PracticalDL/Practical-Deep-Learning-Book/blob/master/code/chapter-4/2-similarity-search-level-1.ipynb\n",
    "\n",
    "def tsne_to_grid_plotter_manual(x, y, style_idxs):\n",
    "\n",
    "    S = 2000\n",
    "    s = 100\n",
    "    x = (x - min(x)) / (max(x) - min(x))\n",
    "    y = (y - min(y)) / (max(y) - min(y))\n",
    "    \n",
    "    x_values = []\n",
    "    y_values = []\n",
    "    idx_plot = []\n",
    "    x_y_dict = {}\n",
    "\n",
    "    for i, idx in enumerate(style_idxs):\n",
    "        a = np.ceil(x[i] * (S - s))\n",
    "        b = np.ceil(y[i] * (S - s))\n",
    "        a = int(a - np.mod(a, s))\n",
    "        b = int(b - np.mod(b, s))\n",
    "        \n",
    "        if str(a) + \"|\" + str(b) in x_y_dict:\n",
    "            continue\n",
    "            \n",
    "        x_y_dict[str(a) + \"|\" + str(b)] = 1\n",
    "        x_values.append(a)\n",
    "        y_values.append(b)\n",
    "        idx_plot.append(idx)\n",
    "        \n",
    "    fig, axis = plt.subplots()\n",
    "    fig.set_size_inches(22, 22, forward=True)\n",
    "    plot_images_in_2d(x_values, y_values, idx_plot, zoom=.58, axis=axis)\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "    plt.savefig('...')\n",
    "    plt.show()\n",
    "    \n",
    "    return (x_values, y_values, idx_plot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Return and plot t-SNE-results\n",
    "\n",
    "x, y, idxs = tsne_to_grid_plotter_manual(tsne_results[:, 0], tsne_results[:, 1], top)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
