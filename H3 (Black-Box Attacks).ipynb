{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn.functional as F\n",
    "import torchvision as tv\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ROOT_DIR = \"\"\n",
    "IMG_SIZE = 64\n",
    "BATCH_SIZE = 128\n",
    "LATENT_DIMS = 16\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_csv = pd.read_csv(ROOT_DIR + \"Train.csv\")\n",
    "test_csv = pd.read_csv(ROOT_DIR + \"Test.csv\")\n",
    "\n",
    "train_files = train_csv[[\"Path\", \"ClassId\"]]\n",
    "test_files = test_csv[[\"Path\", \"ClassId\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfms = tv.transforms.Compose([tv.transforms.Resize((IMG_SIZE, IMG_SIZE)), tv.transforms.ToTensor()])\n",
    "filenames = [os.path.join(dirpath,filename) for dirpath, _, filenames in os.walk(ROOT_DIR + \"Train/\") for filename in filenames if filename.endswith('.png')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data into memory\n",
    "\n",
    "file_arr = []\n",
    "for i in tqdm(range(len(filenames))):\n",
    "    image = Image.open(filenames[i])\n",
    "    tens = tfms(image)\n",
    "    conv_filename = filenames[i].split(\"gtsrb/\")[-1]\n",
    "    class_id = int(train_files[train_files[\"Path\"] == conv_filename][\"ClassId\"].astype(int))\n",
    "    tens_id_arr = [tens, class_id]\n",
    "    file_arr.append(tens_id_arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make sure that classes are mixed before splitting array into train and validation set\n",
    "\n",
    "random.shuffle(file_arr)\n",
    "\n",
    "train_files = file_arr[:-1000]\n",
    "valid_files = file_arr[-1000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TSDataset(Dataset):\n",
    "    def __init__(self, files, transform=None):\n",
    "        self.files = files\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "            \n",
    "        x = self.files[idx][0]\n",
    "        label = self.files[idx][1]\n",
    "            \n",
    "        return x, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = TSDataset(train_files, tfms)\n",
    "valid_data = TSDataset(valid_files, tfms) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(training_data, batch_size=BATCH_SIZE, shuffle=True)\n",
    "# shuffle = false to be able to compare output(-improvements) during training\n",
    "valid_dataloader = DataLoader(valid_data, batch_size=BATCH_SIZE, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Flatten(nn.Module):\n",
    "    def forward(self, input):\n",
    "        return input.view(input.size(0), -1)\n",
    "\n",
    "\n",
    "class UnFlatten(nn.Module):\n",
    "    def forward(self, input, size=1024):\n",
    "        return input.view(input.size(0), size, 1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.kaggle.com/code/muhammad4hmed/anime-vae/notebook\n",
    "\n",
    "class CVAE(nn.Module):\n",
    "    def __init__(self, image_channels=3, h_dim=1024, z_dim=16):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv2d(image_channels, 32, kernel_size=4, stride=2),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 64, kernel_size=4, stride=2),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 128, kernel_size=4, stride=2),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(128, 256, kernel_size=4, stride=2),\n",
    "            nn.ReLU(),\n",
    "            Flatten()\n",
    "        )\n",
    "        \n",
    "        self.signclass_embedding = nn.Embedding(43, 10)\n",
    "        \n",
    "        self.h2mu = nn.Linear(h_dim, z_dim)\n",
    "        self.h2sigma = nn.Linear(h_dim, z_dim)\n",
    "        self.z2h = nn.Linear(z_dim + 10, h_dim)\n",
    "        \n",
    "        self.decoder = nn.Sequential(\n",
    "            UnFlatten(),\n",
    "            nn.ConvTranspose2d(h_dim, 128, kernel_size=5, stride=2),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(128, 64, kernel_size=5, stride=2),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(64, 32, kernel_size=6, stride=2),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(32, image_channels, kernel_size=6, stride=2),\n",
    "            nn.Sigmoid(),\n",
    "        )\n",
    "        \n",
    "    # Enforce latent space well-formedness by ijnecting random gaussian noise    \n",
    "    def reparameterize(self, mu, logvar):          \n",
    "        std = logvar.mul(0.5).exp_()\n",
    "        eps = torch.randn(*mu.size()).to(DEVICE)\n",
    "        z = mu + std * eps\n",
    "        return z\n",
    "    \n",
    "    def bottleneck(self, h, label):\n",
    "        mu = self.h2mu(h)\n",
    "        logvar = self.h2sigma(h)\n",
    "        z = self.reparameterize(mu, logvar)\n",
    "        return z, mu, logvar\n",
    "        \n",
    "    def encode(self, x, label):\n",
    "        return self.bottleneck(self.encoder(x), label)[0]\n",
    "\n",
    "    def decode(self, z):\n",
    "        return self.decoder(self.z2h(z))\n",
    "    \n",
    "    def forward(self, x, label):\n",
    "        h = self.encoder(x)\n",
    "        z_small, mu, logvar = self.bottleneck(h, label)     \n",
    "        signclass = self.signclass_embedding(label.long())\n",
    "        signclass = signclass.squeeze(dim=1)\n",
    "        z_small_cat = torch.cat([z_small, signclass], dim=1)\n",
    "        z = self.z2h(z_small_cat)\n",
    "        return self.decoder(z), mu, logvar, z_small, z"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transfer test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Benchmark classifier\n",
    "# https://github.com/poojahira/gtsrb-pytorch\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        \n",
    "        # CNN layers\n",
    "        self.conv1 = nn.Conv2d(3, 100, kernel_size=5)\n",
    "        self.bn1 = nn.BatchNorm2d(100)\n",
    "        self.conv2 = nn.Conv2d(100, 150, kernel_size=3)\n",
    "        self.bn2 = nn.BatchNorm2d(150)\n",
    "        self.conv3 = nn.Conv2d(150, 250, kernel_size=3)\n",
    "        self.bn3 = nn.BatchNorm2d(250)\n",
    "        self.conv_drop = nn.Dropout2d()\n",
    "        self.fc1 = nn.Linear(250*2*2, 350)\n",
    "        self.fc2 = nn.Linear(350, nclasses)\n",
    "\n",
    "        self.localization = nn.Sequential(\n",
    "            nn.Conv2d(3, 8, kernel_size=7),\n",
    "            nn.MaxPool2d(2, stride=2),\n",
    "            nn.ReLU(True),\n",
    "            nn.Conv2d(8, 10, kernel_size=5),\n",
    "            nn.MaxPool2d(2, stride=2),\n",
    "            nn.ReLU(True)\n",
    "            )\n",
    "\n",
    "        # Regressor for the 3 * 2 affine matrix\n",
    "        self.fc_loc = nn.Sequential(\n",
    "            nn.Linear(10 * 4 * 4, 32),\n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(32, 3 * 2)\n",
    "            )\n",
    "   \n",
    "        # Initialize the weights/bias with identity transformation\n",
    "        self.fc_loc[2].weight.data.zero_()\n",
    "        self.fc_loc[2].bias.data.copy_(torch.tensor([1, 0, 0, 0, 1, 0], dtype=torch.float))\n",
    "\n",
    "\n",
    "    # Spatial transformer network forward function\n",
    "    def stn(self, x):\n",
    "        xs = self.localization(x)\n",
    "        xs = xs.view(-1, 10 * 4 * 4)\n",
    "        theta = self.fc_loc(xs)\n",
    "        theta = theta.view(-1, 2, 3)\n",
    "        grid = F.affine_grid(theta, x.size())\n",
    "        x = F.grid_sample(x, grid)\n",
    "        return x\n",
    "\n",
    "    def forward(self, x):\n",
    "        # transform the input\n",
    "        x = F.interpolate(x, size=(32,32), mode='bilinear')\n",
    "        x = self.stn(x)\n",
    "\n",
    "        # Perform forward pass\n",
    "        x = self.bn1(F.max_pool2d(F.leaky_relu(self.conv1(x)),2))\n",
    "        x = self.conv_drop(x)\n",
    "        x = self.bn2(F.max_pool2d(F.leaky_relu(self.conv2(x)),2))\n",
    "        x = self.conv_drop(x)\n",
    "        x = self.bn3(F.max_pool2d(F.leaky_relu(self.conv3(x)),2))\n",
    "        x = self.conv_drop(x)\n",
    "        x = x.view(-1, 250*2*2)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.dropout(x, training=self.training)\n",
    "        x = self.fc2(x)\n",
    "        return F.log_softmax(x, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Local classifier\n",
    "\n",
    "class Classifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Classifier, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 10, kernel_size=5)\n",
    "        self.conv2 = nn.Conv2d(10, 20, kernel_size=5)\n",
    "        self.conv3 = nn.Conv2d(20, 20, kernel_size=5)\n",
    "        self.conv2_drop = nn.Dropout2d()\n",
    "        self.fc1 = nn.Linear(320, 50)\n",
    "        self.fc2 = nn.Linear(50, 43)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(F.max_pool2d(self.conv1(x), 2))\n",
    "        x = F.relu(F.max_pool2d(self.conv2_drop(self.conv2(x)), 2))\n",
    "        x = F.relu(F.max_pool2d(self.conv2_drop(self.conv3(x)), 2))\n",
    "        x = x.view(-1, 320)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.dropout(x, training=self.training)\n",
    "        x = self.fc2(x)\n",
    "        return F.log_softmax(x, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pre-trained models for classifier and cvae\n",
    "\n",
    "classifier_local = Classifier()\n",
    "classifier_bench = Net()\n",
    "cvae = CVAE()\n",
    "\n",
    "classifier_local.eval()\n",
    "classifier_bench.eval()\n",
    "cvae.eval()\n",
    "\n",
    "classifier_local.load_state_dict(torch.load(\"\"))\n",
    "classifier_bench.load_state_dict(torch.load(\"\"))\n",
    "cvae.load_state_dict(torch.load(\"\"))\n",
    "\n",
    "classifier_local.to(DEVICE);\n",
    "classifier_bench.to(DEVICE);\n",
    "cvae.to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensemble architecture (combining cvae and classifier)\n",
    "\n",
    "class Ensemble(nn.Module):\n",
    "    def __init__(self, embeddings, upscaler, decoder, classifier):\n",
    "        super(Ensemble, self).__init__()\n",
    "        self.embeddings = embeddings\n",
    "        self.upscaler = upscaler\n",
    "        self.decoder = decoder\n",
    "        self.classifier = classifier\n",
    "        \n",
    "    def forward(self, z, label):\n",
    "        enc_label = self.embeddings(label.long())\n",
    "        enc_label = enc_label.squeeze(dim=1)\n",
    "        x = torch.cat((z, enc_label), dim=1)\n",
    "        x = self.upscaler(x)\n",
    "        x = self.decoder(x)\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "    \n",
    "    def get_img(self, z, label):\n",
    "        enc_label = self.embeddings(label.long())\n",
    "        x = torch.cat((z, enc_label), dim=1)\n",
    "        x = self.upscaler(x)\n",
    "        x = self.decoder(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load cvae and classifier into ensemble\n",
    "\n",
    "embeddings, upscaler, decoder = cvae.extract_model()\n",
    "ensemble = Ensemble(embeddings, upscaler, decoder, classifier)\n",
    "ensemble.to(DEVICE);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate adversarial example\n",
    "# https://adversarial-ml-tutorial.org/adversarial_training/\n",
    "\n",
    "def pgd_linf(model, X, y, epsilon, alpha, num_iter):\n",
    "    delta = torch.zeros_like(X, requires_grad=True)\n",
    "    for t in range(num_iter):\n",
    "        pred = model(X + delta, y)\n",
    "        loss = nn.CrossEntropyLoss()(pred, y.squeeze(dim=1))\n",
    "        loss.backward(retain_graph=True)\n",
    "        delta.data = (delta + alpha*delta.grad.detach().sign()).clamp(-epsilon,epsilon)\n",
    "        delta.grad.zero_()\n",
    "    return delta.detach()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cvae_data, cvae_labels = next(iter(train_dataloader))\n",
    "cvae_data, cvae_labels = cvae_data.to(DEVICE), cvae_labels.to(DEVICE)\n",
    "cvae_labels = cvae_labels.unsqueeze(dim=1)\n",
    "cvae_data.shape, cvae_labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate adversarial perturbations\n",
    "\n",
    "mu_local = mu\n",
    "delta = pgd_linf(ensemble, mu_, cvae_labels, epsilon=0.2, alpha=2e-2, num_iter=50)\n",
    "yp = ensemble(mu_local + delta, cvae_labels)\n",
    "prob_orig = F.softmax(yp)\n",
    "\n",
    "mu_adv = mu_local + delta\n",
    "\n",
    "danger_array = prob_orig.gather(1, cvae_labels).squeeze(dim=1)\n",
    "l2_array = torch.linalg.norm(mu_local, dim=1, ord=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the 200 most dangerous examples\n",
    "\n",
    "danger_order = torch.argsort(danger_array, dim=0)\n",
    "top = danger_order[:200]\n",
    "\n",
    "ordered_200 = danger_array[danger_order][:200]\n",
    "ordered_200 = ordered_200.cpu().detach()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the outputs of the predicted true class probabilities of the 200 most adversarial examples (ordered_200) and the examples themselves\n",
    "# Perform the same procedure for the other classifier\n",
    "# Plot the transfered losses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Investigate extremes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot magnitudes\n",
    "plt.scatter(linf_array, danger_array, c =\"blue\")\n",
    "plt.xlabel('L-Infinity Norm')\n",
    "plt.ylabel('Flipped Classes (of 43)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mu_range = torch.zeros((160, 16)).to(DEVICE)\n",
    "\n",
    "# for every dimension, insert the range -X SDs to X SD\n",
    "for i in range(16):\n",
    "    tens = torch.range(-3.25, 3.5, 0.75).to(DEVICE)\n",
    "    mu_range[i*10:(i+1)*10, i] = tens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot all dimensions (here: 16 along specified range)\n",
    "\n",
    "yp = ensemble(mu_range, labels)\n",
    "\n",
    "imgs = ensemble.get_img((mu_range), labels.squeeze(dim=1))\n",
    "imgs = imgs.detach().cpu().numpy()\n",
    "imgs = imgs.transpose(0, 2, 3, 1)\n",
    "plot_images(imgs, labels, yp, 16, 10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
