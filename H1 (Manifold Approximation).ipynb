{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn.functional as F\n",
    "import torchvision as tv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ROOT_DIR = \"\"\n",
    "IMG_SIZE = 64\n",
    "BATCH_SIZE = 128\n",
    "LATENT_DIMS = 16\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_csv = pd.read_csv(ROOT_DIR + \"Train.csv\")\n",
    "test_csv = pd.read_csv(ROOT_DIR + \"Test.csv\")\n",
    "\n",
    "train_files = train_csv[[\"Path\", \"ClassId\"]]\n",
    "test_files = test_csv[[\"Path\", \"ClassId\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfms = tv.transforms.Compose([tv.transforms.Resize((IMG_SIZE, IMG_SIZE)), tv.transforms.ToTensor()])\n",
    "filenames = [os.path.join(dirpath,filename) for dirpath, _, filenames in os.walk(ROOT_DIR + \"Train/\") for filename in filenames if filename.endswith('.png')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data into memory\n",
    "\n",
    "file_arr = []\n",
    "for i in tqdm(range(len(filenames))):\n",
    "    image = Image.open(filenames[i])\n",
    "    tens = tfms(image)\n",
    "    conv_filename = filenames[i].split(\"gtsrb/\")[-1]\n",
    "    class_id = int(train_files[train_files[\"Path\"] == conv_filename][\"ClassId\"].astype(int))\n",
    "    tens_id_arr = [tens, class_id]\n",
    "    file_arr.append(tens_id_arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make sure that classes are mixed before splitting array into train and validation set\n",
    "\n",
    "random.shuffle(file_arr)\n",
    "\n",
    "train_files = file_arr[:-1000]\n",
    "valid_files = file_arr[-1000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TSDataset(Dataset):\n",
    "    def __init__(self, files, transform=None):\n",
    "        self.files = files\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "            \n",
    "        x = self.files[idx][0]\n",
    "        label = self.files[idx][1]\n",
    "            \n",
    "        return x, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = TSDataset(train_files, tfms)\n",
    "valid_data = TSDataset(valid_files, tfms) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(training_data, batch_size=BATCH_SIZE, shuffle=True)\n",
    "# shuffle = false to be able to compare output(-improvements) during training\n",
    "valid_dataloader = DataLoader(valid_data, batch_size=BATCH_SIZE, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sanity check\n",
    "\n",
    "data, labels = next(iter(valid_dataloader))\n",
    "n_cols = 8\n",
    "n_rows = 4\n",
    "\n",
    "fig = plt.figure(figsize=(25, 16))\n",
    "for i, img in enumerate(data):\n",
    "    \n",
    "    if (n_cols*n_rows) >= (i + 1):\n",
    "        ax = fig.add_subplot(n_rows, n_cols, i + 1)\n",
    "        img = img.numpy().transpose(1, 2, 0)\n",
    "        plt.axis('off')\n",
    "        plt.imshow((img))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Flatten(nn.Module):\n",
    "    def forward(self, input):\n",
    "        return input.view(input.size(0), -1)\n",
    "\n",
    "\n",
    "class UnFlatten(nn.Module):\n",
    "    def forward(self, input, size=1024):\n",
    "        return input.view(input.size(0), size, 1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.kaggle.com/code/muhammad4hmed/anime-vae/notebook\n",
    "\n",
    "class CVAE(nn.Module):\n",
    "    def __init__(self, image_channels=3, h_dim=1024, z_dim=16):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv2d(image_channels, 32, kernel_size=4, stride=2),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 64, kernel_size=4, stride=2),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 128, kernel_size=4, stride=2),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(128, 256, kernel_size=4, stride=2),\n",
    "            nn.ReLU(),\n",
    "            Flatten()\n",
    "        )\n",
    "        \n",
    "        self.signclass_embedding = nn.Embedding(43, 10)\n",
    "        \n",
    "        self.h2mu = nn.Linear(h_dim, z_dim)\n",
    "        self.h2sigma = nn.Linear(h_dim, z_dim)\n",
    "        self.z2h = nn.Linear(z_dim + 10, h_dim)\n",
    "        \n",
    "        self.decoder = nn.Sequential(\n",
    "            UnFlatten(),\n",
    "            nn.ConvTranspose2d(h_dim, 128, kernel_size=5, stride=2),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(128, 64, kernel_size=5, stride=2),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(64, 32, kernel_size=6, stride=2),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(32, image_channels, kernel_size=6, stride=2),\n",
    "            nn.Sigmoid(),\n",
    "        )\n",
    "        \n",
    "    # Enforce latent space well-formedness by jinecting random gaussian noise    \n",
    "    def reparameterize(self, mu, logvar):          \n",
    "        std = logvar.mul(0.5).exp_()\n",
    "        eps = torch.randn(*mu.size()).to(DEVICE)\n",
    "        z = mu + std * eps\n",
    "        return z\n",
    "    \n",
    "    def bottleneck(self, h, label):\n",
    "        mu = self.h2mu(h)\n",
    "        logvar = self.h2sigma(h)\n",
    "        z = self.reparameterize(mu, logvar)\n",
    "        return z, mu, logvar\n",
    "        \n",
    "    def encode(self, x, label):\n",
    "        return self.bottleneck(self.encoder(x), label)[0]\n",
    "\n",
    "    def decode(self, z):\n",
    "        return self.decoder(self.z2h(z))\n",
    "    \n",
    "    def forward(self, x, label):\n",
    "        h = self.encoder(x)\n",
    "        z_small, mu, logvar = self.bottleneck(h, label)     \n",
    "        signclass = self.signclass_embedding(label.long())\n",
    "        signclass = signclass.squeeze(dim=1)\n",
    "        z_small_cat = torch.cat([z_small, signclass], dim=1)\n",
    "        z = self.z2h(z_small_cat)\n",
    "        return self.decoder(z), mu, logvar, z_small, z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CVAE()\n",
    "model.to(DEVICE)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=5e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vae_loss(recon_x, x, mu, logvar):\n",
    "    BCE = F.binary_cross_entropy(recon_x.view(-1, IMG_SIZE*IMG_SIZE*3),\n",
    "                                 x.view(-1, IMG_SIZE*IMG_SIZE*3), reduction='sum')\n",
    "    KLD = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "    return BCE + KLD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def num_params(model):\n",
    "    return sum(p.numel() for p in model.parameters())\n",
    "\n",
    "num_params(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_interm_results():\n",
    "    x, label = next(iter(valid_dataloader))\n",
    "\n",
    "    if torch.cuda.is_available():\n",
    "        x, label = x.cuda(), label.type(torch.FloatTensor).unsqueeze(dim=1).cuda()\n",
    "\n",
    "    imgs, mu, logvar, _, _ = model(x, label)\n",
    "    imgs = imgs.detach().cpu()          \n",
    "\n",
    "    n_cols = 8\n",
    "    n_rows = 4\n",
    "\n",
    "    fig = plt.figure(figsize=(25, 16))\n",
    "    for i in range(n_cols*n_rows):\n",
    "\n",
    "        if (n_cols*n_rows) >= (i + 1):\n",
    "            ax = fig.add_subplot(n_rows, n_cols, i + 1)\n",
    "            img = imgs[i]\n",
    "            img = img.permute(1, 2, 0)\n",
    "            plt.axis('off')\n",
    "            plt.imshow((img))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 2000\n",
    "epoch_train_losses = []\n",
    "epoch_valid_losses = []\n",
    "\n",
    "for epoch in tqdm(range(epochs)):\n",
    "    batch_train_losses = []\n",
    "    batch_valid_losses = []\n",
    "    \n",
    "    model.train()\n",
    "    for data, label in train_dataloader:\n",
    "        optimizer.zero_grad()\n",
    "        if torch.cuda.is_available():\n",
    "            data, label = data.cuda(), label.type(torch.FloatTensor).unsqueeze(dim=1).cuda()\n",
    "\n",
    "        recon_batch, mu, logvar, _, _ = model(data, label)  \n",
    "        loss = vae_loss(recon_batch, data, mu, logvar)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        batch_train_losses.append(loss.item()/data.shape[0])\n",
    "    epoch_train_losses.append(np.mean(batch_train_losses))\n",
    "    \n",
    "    model.eval()  \n",
    "    for data, label in valid_dataloader:\n",
    "        if torch.cuda.is_available():\n",
    "            data, label = data.cuda(), label.type(torch.FloatTensor).unsqueeze(dim=1).cuda()\n",
    "        \n",
    "        recon_x, mu, logvar, _, _ = model(data, label)\n",
    "        loss = vae_loss(recon_x, data, mu, logvar)\n",
    "        \n",
    "        batch_valid_losses.append(loss.item()/data.shape[0])\n",
    "    epoch_valid_losses.append(np.mean(batch_valid_losses))\n",
    "    \n",
    "    \n",
    "    if epoch % 100 == 0 :\n",
    "        torch.save(model.state_dict(), \"cvae_epoch_{}.pth\".format(epoch))\n",
    "        \n",
    "    print(f'Epoch {epoch+1} \\t\\t Training Loss: {np.mean(epoch_train_losses)} \\t\\t Validation Loss: {np.mean(epoch_valid_losses)}')\n",
    "    \n",
    "    plt.plot(epoch_train_losses, label = \"train_loss\")\n",
    "    plt.plot(epoch_valid_losses, label = \"valid_loss\")\n",
    "\n",
    "    plot_interm_results()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Validation loss comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Save the valid loss array and start training with new latent space value from scratch\n",
    "\n",
    "plt.plot(dim64 label = \"64\")\n",
    "plt.plot(dim32, label = \"32\")\n",
    "plt.plot(dim16, label = \"16\")\n",
    "plt.plot(dim8, label = \"8\")\n",
    "plt.plot(dim4, label = \"4\")\n",
    "\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend(title=\"Dimensions\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test error comparison (Benchmark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/poojahira/gtsrb-pytorch\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        \n",
    "        # CNN layers\n",
    "        self.conv1 = nn.Conv2d(3, 100, kernel_size=5)\n",
    "        self.bn1 = nn.BatchNorm2d(100)\n",
    "        self.conv2 = nn.Conv2d(100, 150, kernel_size=3)\n",
    "        self.bn2 = nn.BatchNorm2d(150)\n",
    "        self.conv3 = nn.Conv2d(150, 250, kernel_size=3)\n",
    "        self.bn3 = nn.BatchNorm2d(250)\n",
    "        self.conv_drop = nn.Dropout2d()\n",
    "        self.fc1 = nn.Linear(250*2*2, 350)\n",
    "        self.fc2 = nn.Linear(350, nclasses)\n",
    "\n",
    "        self.localization = nn.Sequential(\n",
    "            nn.Conv2d(3, 8, kernel_size=7),\n",
    "            nn.MaxPool2d(2, stride=2),\n",
    "            nn.ReLU(True),\n",
    "            nn.Conv2d(8, 10, kernel_size=5),\n",
    "            nn.MaxPool2d(2, stride=2),\n",
    "            nn.ReLU(True)\n",
    "            )\n",
    "\n",
    "        # Regressor for the 3 * 2 affine matrix\n",
    "        self.fc_loc = nn.Sequential(\n",
    "            nn.Linear(10 * 4 * 4, 32),\n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(32, 3 * 2)\n",
    "            )\n",
    "   \n",
    "        # Initialize the weights/bias with identity transformation\n",
    "        self.fc_loc[2].weight.data.zero_()\n",
    "        self.fc_loc[2].bias.data.copy_(torch.tensor([1, 0, 0, 0, 1, 0], dtype=torch.float))\n",
    "\n",
    "\n",
    "    # Spatial transformer network forward function\n",
    "    def stn(self, x):\n",
    "        xs = self.localization(x)\n",
    "        xs = xs.view(-1, 10 * 4 * 4)\n",
    "        theta = self.fc_loc(xs)\n",
    "        theta = theta.view(-1, 2, 3)\n",
    "        grid = F.affine_grid(theta, x.size())\n",
    "        x = F.grid_sample(x, grid)\n",
    "        return x\n",
    "\n",
    "    def forward(self, x):\n",
    "        # transform the input\n",
    "        x = F.interpolate(x, size=(32,32), mode='bilinear')\n",
    "        x = self.stn(x)\n",
    "\n",
    "        # Perform forward pass\n",
    "        x = self.bn1(F.max_pool2d(F.leaky_relu(self.conv1(x)),2))\n",
    "        x = self.conv_drop(x)\n",
    "        x = self.bn2(F.max_pool2d(F.leaky_relu(self.conv2(x)),2))\n",
    "        x = self.conv_drop(x)\n",
    "        x = self.bn3(F.max_pool2d(F.leaky_relu(self.conv3(x)),2))\n",
    "        x = self.conv_drop(x)\n",
    "        x = x.view(-1, 250*2*2)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.dropout(x, training=self.training)\n",
    "        x = self.fc2(x)\n",
    "        return F.log_softmax(x, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = Net()\n",
    "classifier.load_state_dict(torch.load(\"...\"))\n",
    "classifier.to(DEVICE)\n",
    "classifier.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correct = 0.\n",
    "nr = 0.01\n",
    "model.eval()  \n",
    "for data, label in test_dataloader:\n",
    "    if torch.cuda.is_available():\n",
    "        data, label = data.cuda(), label.type(torch.FloatTensor).unsqueeze(dim=1).cuda()\n",
    "    recon_x, mu, logvar, _, _ = model(data, label)\n",
    "    recon_x = data_transforms(recon_x)\n",
    "    output = classifier(recon_x)\n",
    "    output = torch.argmax(output, dim=1)\n",
    "    label = label.squeeze(dim=1)\n",
    "    correct += (output == label).float().sum()\n",
    "    nr += data.shape[0]\n",
    "\n",
    "accuracy = 100 * correct / nr\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot ranges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_images(X, y, yp, M, N):\n",
    "    f, ax = plt.subplots(M, N, sharex=True, sharey=True, figsize=(N,M*1.3))\n",
    "    prob = F.softmax(yp)\n",
    "    print(prob.shape)\n",
    "    prob = prob.gather(1, y)\n",
    "    for i in range(M):\n",
    "        for j in range(N):\n",
    "            ax[i][j].imshow(X[i*N+j])\n",
    "            title = ax[i][j].set_title(\"{:.2f}\".format(prob[i*N+j].item()))\n",
    "            plt.setp(title, color=('g' if yp[i*N+j].max(dim=0)[1] == y[i*N+j] else 'r'))\n",
    "            ax[i][j].set_axis_off()\n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensemble architecture (combining cvae and classifier)\n",
    "\n",
    "class Ensemble(nn.Module):\n",
    "    def __init__(self, embeddings, upscaler, decoder, classifier):\n",
    "        super(Ensemble, self).__init__()\n",
    "        self.embeddings = embeddings\n",
    "        self.upscaler = upscaler\n",
    "        self.decoder = decoder\n",
    "        self.classifier = classifier\n",
    "        \n",
    "    def forward(self, z, label):\n",
    "        enc_label = self.embeddings(label.long())\n",
    "        enc_label = enc_label.squeeze(dim=1)\n",
    "        x = torch.cat((z, enc_label), dim=1)\n",
    "        x = self.upscaler(x)\n",
    "        x = self.decoder(x)\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "    \n",
    "    def get_img(self, z, label):\n",
    "        enc_label = self.embeddings(label.long())\n",
    "        x = torch.cat((z, enc_label), dim=1)\n",
    "        x = self.upscaler(x)\n",
    "        x = self.decoder(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load cvae and classifier into ensemble\n",
    "\n",
    "embeddings, upscaler, decoder = cvae.extract_model()\n",
    "ensemble = Ensemble(embeddings, upscaler, decoder, classifier)\n",
    "ensemble.to(DEVICE);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cvae_data, cvae_labels = next(iter(train_dataloader))\n",
    "cvae_data, cvae_labels = cvae_data.to(DEVICE), cvae_labels.to(DEVICE)\n",
    "cvae_labels = cvae_labels.unsqueeze(dim=1)\n",
    "cvae_data.shape, cvae_labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mu_range = torch.zeros((160, 16)).to(DEVICE)\n",
    "\n",
    "# for every dimension, insert the range -X SDs to X SD\n",
    "for i in range(16):\n",
    "    tens = torch.range(-3.25, 3.5, 0.75).to(DEVICE)\n",
    "    mu_range[i*10:(i+1)*10, i] = tens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot all dimensions (here: 16 along specified range)\n",
    "\n",
    "yp = ensemble(mu_range, labels)\n",
    "\n",
    "imgs = ensemble.get_img((mu_range), labels.squeeze(dim=1))\n",
    "imgs = imgs.detach().cpu().numpy()\n",
    "imgs = imgs.transpose(0, 2, 3, 1)\n",
    "plot_images(imgs, labels, yp, 16, 10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
