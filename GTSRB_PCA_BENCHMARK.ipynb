{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Bad key \"text.kerning_factor\" on line 4 in\n",
      "/home/christoph/anaconda3/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test_patch.mplstyle.\n",
      "You probably need to get an updated matplotlibrc file from\n",
      "https://github.com/matplotlib/matplotlib/blob/v3.1.3/matplotlibrc.template\n",
      "or from the matplotlib source distribution\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "from PIL import Image\n",
    "\n",
    "import pickle\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "import torchvision as tv\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "ROOT_DIR = \"/home/christoph/Desktop/ma/data/gtsrb/\"\n",
    "IMG_SIZE = 64\n",
    "BATCH_SIZE = 128\n",
    "LATENT_DIMS = 64\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_csv = pd.read_csv(ROOT_DIR + \"Train.csv\")\n",
    "valid_csv = pd.read_csv(ROOT_DIR + \"Test.csv\")\n",
    "\n",
    "train_files = train_csv[[\"Path\", \"ClassId\"]]\n",
    "valid_files = valid_csv[[\"Path\", \"ClassId\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfms = tv.transforms.Compose([tv.transforms.Resize((IMG_SIZE, IMG_SIZE)), tv.transforms.ToTensor()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "filenames = [os.path.join(dirpath,filename) for dirpath, _, filenames in os.walk(ROOT_DIR + \"Train/\") for filename in filenames if filename.endswith('.png')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9209/9209 [00:02<00:00, 3373.15it/s]\n"
     ]
    }
   ],
   "source": [
    "file_arr = []\n",
    "for i in tqdm(range(len(filenames)-30000)):\n",
    "    if i%10 == 0:\n",
    "        image = Image.open(filenames[i])\n",
    "        tens = tfms(image)\n",
    "        conv_filename = filenames[i].split(\"gtsrb/\")[-1]\n",
    "        class_id = int(train_files[train_files[\"Path\"] == conv_filename][\"ClassId\"].astype(int))\n",
    "        tens_id_arr = [tens, class_id]\n",
    "        file_arr.append(tens_id_arr)\n",
    "    \n",
    "random.shuffle(file_arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TSDataset(Dataset):\n",
    "    def __init__(self, files, transform=None):\n",
    "        self.files = files\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "            \n",
    "        x = self.files[idx][0]\n",
    "        label = self.files[idx][1]\n",
    "            \n",
    "        return x, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = TSDataset(file_arr, tfms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(training_data, batch_size=BATCH_SIZE, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "nclasses = 43"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Classifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Classifier, self).__init__()\n",
    "        \n",
    "        # CNN layers\n",
    "        self.conv1 = nn.Conv2d(3, 100, kernel_size=5)\n",
    "        self.bn1 = nn.BatchNorm2d(100)\n",
    "        self.conv2 = nn.Conv2d(100, 150, kernel_size=3)\n",
    "        self.bn2 = nn.BatchNorm2d(150)\n",
    "        self.conv3 = nn.Conv2d(150, 250, kernel_size=3)\n",
    "        self.bn3 = nn.BatchNorm2d(250)\n",
    "        self.conv_drop = nn.Dropout2d()\n",
    "        self.fc1 = nn.Linear(250*2*2, 350)\n",
    "        self.fc2 = nn.Linear(350, nclasses)\n",
    "\n",
    "        self.localization = nn.Sequential(\n",
    "            nn.Conv2d(3, 8, kernel_size=7),\n",
    "            nn.MaxPool2d(2, stride=2),\n",
    "            nn.ReLU(True),\n",
    "            nn.Conv2d(8, 10, kernel_size=5),\n",
    "            nn.MaxPool2d(2, stride=2),\n",
    "            nn.ReLU(True)\n",
    "            )\n",
    "\n",
    "        # Regressor for the 3 * 2 affine matrix\n",
    "        self.fc_loc = nn.Sequential(\n",
    "            nn.Linear(10 * 4 * 4, 32),\n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(32, 3 * 2)\n",
    "            )\n",
    "   \n",
    "        # Initialize the weights/bias with identity transformation\n",
    "        self.fc_loc[2].weight.data.zero_()\n",
    "        self.fc_loc[2].bias.data.copy_(torch.tensor([1, 0, 0, 0, 1, 0], dtype=torch.float))\n",
    "\n",
    "\n",
    "    # Spatial transformer network forward function\n",
    "    def stn(self, x):\n",
    "        xs = self.localization(x)\n",
    "        xs = xs.view(-1, 10 * 4 * 4)\n",
    "        theta = self.fc_loc(xs)\n",
    "        theta = theta.view(-1, 2, 3)\n",
    "        grid = F.affine_grid(theta, x.size())\n",
    "        x = F.grid_sample(x, grid)\n",
    "        return x\n",
    "\n",
    "    def forward(self, x):\n",
    "        # transform the input\n",
    "        x = self.stn(x)\n",
    "\n",
    "        # Perform forward pass\n",
    "        x = self.bn1(F.max_pool2d(F.leaky_relu(self.conv1(x)),2))\n",
    "        x = self.conv_drop(x)\n",
    "        x = self.bn2(F.max_pool2d(F.leaky_relu(self.conv2(x)),2))\n",
    "        x = self.conv_drop(x)\n",
    "        x = self.bn3(F.max_pool2d(F.leaky_relu(self.conv3(x)),2))\n",
    "        x = self.conv_drop(x)\n",
    "        x = x.view(-1, 250*2*2)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.dropout(x, training=self.training)\n",
    "        x = self.fc2(x)\n",
    "        return F.log_softmax(x, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CVAE architecture\n",
    "\n",
    "class Flatten(nn.Module):\n",
    "    def forward(self, input):\n",
    "        return input.view(input.size(0), -1)\n",
    "\n",
    "\n",
    "class UnFlatten(nn.Module):\n",
    "    def forward(self, input, size=1024):\n",
    "        return input.view(input.size(0), size, 1, 1)\n",
    "\n",
    "class CVAE(nn.Module):\n",
    "    def __init__(self, image_channels=3, h_dim=1024, z_dim=32):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv2d(image_channels, 32, kernel_size=4, stride=2),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 64, kernel_size=4, stride=2),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 128, kernel_size=4, stride=2),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(128, 256, kernel_size=4, stride=2),\n",
    "            nn.ReLU(),\n",
    "            Flatten()\n",
    "        )\n",
    "        \n",
    "        self.signclass_embedding = nn.Embedding(43, 20)\n",
    "        \n",
    "        self.h2mu = nn.Linear(h_dim, z_dim)\n",
    "        self.h2sigma = nn.Linear(h_dim, z_dim)\n",
    "        self.z2h = nn.Linear(z_dim + 20, h_dim)\n",
    "        \n",
    "        self.decoder = nn.Sequential(\n",
    "            UnFlatten(),\n",
    "            nn.ConvTranspose2d(h_dim, 128, kernel_size=5, stride=2),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(128, 64, kernel_size=5, stride=2),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(64, 32, kernel_size=6, stride=2),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(32, image_channels, kernel_size=6, stride=2),\n",
    "            nn.Sigmoid(),\n",
    "        )\n",
    "        \n",
    "        \n",
    "    def reparameterize(self, mu, logvar):\n",
    "        std = logvar.mul(0.5).exp_()\n",
    "        eps = torch.randn(*mu.size()).to(DEVICE)\n",
    "        z = mu + std * eps\n",
    "        return z\n",
    "    \n",
    "    def bottleneck(self, h, label):\n",
    "        mu = self.h2mu(h)\n",
    "        logvar = self.h2sigma(h)\n",
    "\n",
    "        z = self.reparameterize(mu, logvar)\n",
    "        \n",
    "        return z, mu, logvar\n",
    "    \n",
    "    def extract_model(self):\n",
    "        return self.signclass_embedding, self.z2h, self.decoder\n",
    "        \n",
    "    def encode(self, x, label):\n",
    "        return self.bottleneck(self.encoder(x), label)[0]\n",
    "\n",
    "    def decode(self, z):\n",
    "        return self.decoder(self.z2h(z))\n",
    "    \n",
    "    def forward(self, x, label):\n",
    "        h = self.encoder(x)\n",
    "        z_small, mu, logvar = self.bottleneck(h, label)\n",
    "        \n",
    "        signclass = self.signclass_embedding(label.long())\n",
    "        signclass = signclass.squeeze(dim=1)\n",
    "        z_small_cat = torch.cat([z_small, signclass], dim=1)\n",
    "        z = self.z2h(z_small_cat)\n",
    "        return self.decoder(z), mu, logvar, z_small, z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensemble architecture (combining cvae and classifier)\n",
    "\n",
    "class Ensemble(nn.Module):\n",
    "    def __init__(self, embeddings, upscaler, decoder, classifier):\n",
    "        super(Ensemble, self).__init__()\n",
    "        self.embeddings = embeddings\n",
    "        self.upscaler = upscaler\n",
    "        self.decoder = decoder\n",
    "        self.classifier = classifier\n",
    "        self.downscaler = nn.MaxPool2d(2, stride=2)\n",
    "        \n",
    "    def forward(self, z, label):\n",
    "        enc_label = self.embeddings(label.long())\n",
    "        enc_label = enc_label.squeeze(dim=1)\n",
    "        x = torch.cat((z, enc_label), dim=1)\n",
    "        x = self.upscaler(x)\n",
    "        x = self.decoder(x)\n",
    "        x = self.downscaler(x)\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "    \n",
    "    def get_img(self, z, label):\n",
    "        enc_label = self.embeddings(label.long())\n",
    "        x = torch.cat((z, enc_label), dim=1)\n",
    "        x = self.upscaler(x)\n",
    "        x = self.decoder(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pre-trained models for classifier and cvae\n",
    "\n",
    "classifier = Classifier()\n",
    "classifier.eval()\n",
    "cvae = CVAE()\n",
    "\n",
    "classifier.load_state_dict(torch.load(\"gtsrb_benchmark.pth\"))\n",
    "cvae.load_state_dict(torch.load(\"ccvae_32_signs_extract_50.pth\"))\n",
    "\n",
    "classifier.to(DEVICE)\n",
    "cvae.to(DEVICE);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load cvae and classifier into ensemble\n",
    "\n",
    "embeddings, upscaler, decoder = cvae.extract_model()\n",
    "ensemble = Ensemble(embeddings, upscaler, decoder, classifier)\n",
    "ensemble.to(DEVICE);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "np_arr = []\n",
    "for data, label in train_dataloader:\n",
    "        if torch.cuda.is_available():\n",
    "            data, label = data.cuda(), label.type(torch.FloatTensor).unsqueeze(dim=1).cuda()\n",
    "\n",
    "        recon_batch, mu, logvar, _, _ = cvae(data, label)  \n",
    "        to_np = mu.detach().cpu().numpy()\n",
    "        np_arr.append(to_np)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "stacked_tensors = np.vstack(np_arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/home/christoph/Desktop/gtsrb_32dimv2.npy', 'wb') as f:\n",
    "    a = np.save(f, stacked_tensors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/home/christoph/Desktop/gtsrb_32dim.npy', 'rb') as f:\n",
    "    tensors = np.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "### PCA ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Pipeline ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/home/christoph/Desktop/gtsrb_pca.pkl', 'rb') as f:\n",
    "    pca = pickle.load(f);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/home/christoph/Desktop/gtsrb_feats.npy', 'rb') as f:\n",
    "    embs = np.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "ranges = [-1, -0.5, 0, 0.5, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([128, 3, 64, 64]), torch.Size([128, 1]))"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "selected_class = 5\n",
    "ce_loss = nn.CrossEntropyLoss()\n",
    "\n",
    "cvae_data, cvae_labels = next(iter(train_dataloader))\n",
    "cvae_data, cvae_labels = cvae_data.to(DEVICE), cvae_labels.to(DEVICE)\n",
    "cvae_labels = cvae_labels.unsqueeze(dim=1)\n",
    "cvae_data.shape, cvae_labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recon_batch, mu, logvar, data, z = cvae(cvae_data, cvae_labels)\n",
    "mu[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yilog(yihat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_noise_1d(z_orig, label, ranges, dim):\n",
    "    noisy_data = []\n",
    "    probs = []\n",
    "    for step, value in enumerate(ranges):\n",
    "        noise = torch.zeros(1, 5).to(DEVICE)\n",
    "        noise[0, dim] = value\n",
    "        z_compressed = z_orig.unsqueeze(dim=0) + noise\n",
    "        \n",
    "        z_compressed = z_compressed.to(\"cpu\")\n",
    "        \n",
    "        z = pca.inverse_transform(z_compressed)\n",
    "        \n",
    "        z = torch.Tensor(z).to(DEVICE)\n",
    "\n",
    "        pred_logits = ensemble(z, label.unsqueeze(dim=0)) \n",
    "        loss = ce_loss(pred_logits, label)\n",
    "        norm_vals = abs(value)\n",
    "        loss_normed = loss*100/torch.exp(torch.Tensor([norm_vals]).to(DEVICE))\n",
    "        pred_probs = F.softmax(pred_logits)\n",
    "\n",
    "        pred = pred_probs.max(1, keepdim=True)[1][0]\n",
    "        prob = pred_probs.max(1, keepdim=True)[0]\n",
    "\n",
    "        img = ensemble.get_img(z, label)\n",
    "        img = img.squeeze(dim=0)\n",
    "        img = img.detach().cpu().numpy()\n",
    "        img = img.transpose(1, 2, 0)\n",
    "\n",
    "        noisy_data.append(img)\n",
    "        probs.append(prob.item())\n",
    "            \n",
    "    return [noisy_data, np.round(probs, 4)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = torch.Tensor(embs[0]).to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Widget ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ipywidgets as widgets\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from ipywidgets import interact"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def z_to_img(idx):\n",
    "    grid_data = add_noise(mu[idx], cvae_labels[idx], np.linspace(-3, 3, 10), 1)\n",
    "    [row, col], img, prob, loss_normed = grid_data[0]\n",
    "    img = img.detach().cpu().numpy()\n",
    "    img = img.transpose(1, 2, 0)\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_image(x):\n",
    "    x_scaled = np.uint8(255 * (x - x.min()) / x.ptp())\n",
    "    return Image.fromarray(x_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "279f1b73bfc549019744f89035c7799e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatText(value=0.0)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/christoph/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:19: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6c0fe465be0c4f2eb58b0e45647469ec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=49, description='range_step', max=99), Output()), _dom_classes=('widget-…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "a = widgets.FloatText()\n",
    "display(a)\n",
    "\n",
    "def display_sequence(images, probs):\n",
    "    def _show(range_step=(0, len(images)-1)):\n",
    "        a.value = probs[range_step]\n",
    "        return display_image(images[range_step])\n",
    "    return interact(_show)\n",
    "\n",
    "x = torch.Tensor(embs[40]).to(DEVICE)\n",
    "images, probs = add_noise_1d(x, cvae_labels[100], np.linspace(-5, 5, 100), 1)\n",
    "    \n",
    "# prob = probs[0]    \n",
    "\n",
    "display_sequence(images, probs);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dim-0: nächtlich-grell\n",
    "# dim-1: klein-groß und Winkel\n",
    "# dim-2: Kontrast\n",
    "# dim-3: Ausleuchtung\n",
    "# dim-4: Winkel(?) und Ausleuchtung"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
